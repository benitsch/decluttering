{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "D:\\Anaconda\\envs\\tf_gpu\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "D:\\Anaconda\\envs\\tf_gpu\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "D:\\Anaconda\\envs\\tf_gpu\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "D:\\Anaconda\\envs\\tf_gpu\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "D:\\Anaconda\\envs\\tf_gpu\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "D:\\Anaconda\\envs\\tf_gpu\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "C:\\Users\\Administrator\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "C:\\Users\\Administrator\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "C:\\Users\\Administrator\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "C:\\Users\\Administrator\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "C:\\Users\\Administrator\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "C:\\Users\\Administrator\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Flatten, Dropout, Bidirectional, TimeDistributed,Conv1D, MaxPooling1D, Input, concatenate\n",
    "from keras.layers.recurrent import SimpleRNN\n",
    "from keras.layers import GRU, LSTM\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.preprocessing import sequence\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.models import Model\n",
    "from keras.callbacks import EarlyStopping\n",
    "import keras\n",
    "import os\n",
    "import tarfile\n",
    "import re\n",
    "import pandas as pd\n",
    "from numpy import array\n",
    "from keras.preprocessing.text import one_hot\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "import numpy as np\n",
    "from sklearn.model_selection import KFold\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "def RNN(maxlen = 50, max_features = 4590, embed_size =32):\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(max_features, embed_size, input_length=maxlen))\n",
    "#     model.add(Dropout(0.5))\n",
    "    model.add(GRU(16,dropout=0.2))\n",
    "#     model.add(Dense(8, activation='relu'))\n",
    "#     model.add(Dropout(0.5))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    print(model.summary())\n",
    "\n",
    "    return model\n",
    "\n",
    "def CNN(maxlen=50, max_features=4590, embed_size=32):\n",
    "    # Inputs\n",
    "    comment_seq = Input(shape=[maxlen], name='x_seq')\n",
    "\n",
    "    # Embeddings layers\n",
    "    emb_comment = Embedding(max_features, embed_size)(comment_seq)\n",
    "\n",
    "    # conv layers\n",
    "    convs = []\n",
    "#     filter_sizes = [2, 3, 4, 5]\n",
    "    filter_sizes = [2, 3]\n",
    "\n",
    "    for fsz in filter_sizes:\n",
    "        l_conv = Conv1D(filters=10, kernel_size=fsz, activation='relu')(emb_comment)\n",
    "        l_pool = MaxPooling1D(maxlen - fsz + 1)(l_conv)\n",
    "        l_pool = Flatten()(l_pool)\n",
    "        convs.append(l_pool)\n",
    "    merge = concatenate(convs, axis=1)\n",
    "\n",
    "    out = Dropout(0.2)(merge)\n",
    "    output = Dense(16, activation='relu')(out)\n",
    "\n",
    "    output = Dense(units=1, activation='sigmoid')(output)\n",
    "\n",
    "    model = Model([comment_seq], output)\n",
    "    print(model.summary())\n",
    "    return model\n",
    "\n",
    "\n",
    "def embedding(maxlen=50, max_features=4590, embed_size=16):\n",
    "    # define the model\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(max_features, embed_size, input_length=maxlen))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    # summarize the model\n",
    "    print(model.summary())\n",
    "    return model\n",
    "\n",
    "    \n",
    "def model_fit(model,padded_docs,labels,train_index,test_index, epochs = 5):\n",
    "    len_train = len(train_index)\n",
    "    len_test = len(test_index)\n",
    "    x_test =  np.zeros(shape = (len_test,len(padded_docs[0])))\n",
    "    y_test = np.zeros(shape = (len_test))\n",
    "    x_train = np.zeros(shape = (len_train,len(padded_docs[0])))\n",
    "    y_train = np.zeros(shape = (len_train))\n",
    "    i = 0\n",
    "    for tri in train_index:\n",
    "        x_train[i] = padded_docs[tri]\n",
    "        y_train[i] = labels[tri]\n",
    "        i+=1\n",
    "    j = 0\n",
    "    for ti in test_index:\n",
    "        x_test[j] = padded_docs[ti]\n",
    "        y_test[j] =  labels[ti]\n",
    "        j +=1\n",
    "        \n",
    "    print(\"Test set from \",test_index[0], \" to \",test_index[-1])\n",
    "\n",
    "    model.fit(x_train, y_train,epochs=epochs)\n",
    "    eva = model.evaluate(x_test,y_test)\n",
    "    print('loss: ',eva[0])\n",
    "    print('accuracy: ',eva[1])\n",
    "    print('precision: ',eva[2])\n",
    "    print('recall: ',eva[3])\n",
    "    print('f1-score: ',(2*eva[3]*eva[2])/(eva[3]+eva[2]))\n",
    "    return model \n",
    "\n",
    "def model_fit_no_test(model,x_train,y_train, epochs = 5):\n",
    "    \n",
    "    model.fit(x_train, y_train,epochs=epochs)\n",
    "#     print('loss: ',eva[0])\n",
    "#     print('accuracy: ',eva[1])\n",
    "#     print('precision: ',eva[2])\n",
    "#     print('recall: ',eva[3])\n",
    "#     print('f1-score: ',(2*eva[3]*eva[2])/(eva[3]+eva[2]))\n",
    "    return model \n",
    "\n",
    "import numpy\n",
    "def predict_test(model, testData):\n",
    "    test = numpy.reshape(testData,(1,testData.shape[0]))\n",
    "    pr = model.predict(test)\n",
    "    re = []\n",
    "    if (pr >= 0.5):\n",
    "        return (\"yes\")\n",
    "    else:\n",
    "        return (\"no\")\n",
    "    \n",
    "def predict_train(model, testData, truth):\n",
    "    test = numpy.reshape(testData,(1,testData.shape[0]))\n",
    "    pr = model.predict(test)\n",
    "    print(\"==================\")\n",
    "    print(\"truth = \",truth )\n",
    "    print(\"predict = \",pr )\n",
    "    if (pr >= 0.5 and truth == 1) or (pr< 0.5 and truth ==0):\n",
    "        print(\"true\")\n",
    "        return 1\n",
    "    else:\n",
    "        print(\"false\")\n",
    "        return 0\n",
    "    \n",
    "def write_result(predict, outPath,inpath =\"assets/test_set_0520.csv\"):\n",
    "    dataframe_in = pd.read_csv(inpath, na_filter = False)\n",
    "    dataframe_in[\"Expected\"] = predict\n",
    "    dataframe_in.to_csv(outPath)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define documents\n",
    "## read train Data\n",
    "def read_train_Data():\n",
    "    SPL = \"<SPL>\"\n",
    "    path = \"csv/trimedValues2.csv\"\n",
    "    train_file = path\n",
    "    dataframe = pd.read_csv(path, na_filter = False)\n",
    "    docs = []\n",
    "    labels = []\n",
    "    for i,data in dataframe.iterrows():\n",
    "        d = data[\"comment\"]\n",
    "        c = data[\"code\"]\n",
    "        s = []\n",
    "        s =  d+\" \"+SPL+\" \"+c # comment and code together\n",
    "        docs.append(s)\n",
    "        l = 1 if data[\"non-information\"] == \"yes\" else 0\n",
    "        labels.append(l)\n",
    "    # integer encode the documents\n",
    "    vocab_size = 2000\n",
    "    encoded_docs = [one_hot(d, vocab_size) for d in docs]\n",
    "    # pad documents to a max length of 41 words\n",
    "    # The langest sentence contains 200 words but the Average is 21. If  padding all sentence to length of langest one, most of sentence\n",
    "    # will be 0. That is not good for RNN, so we set padding length to 50\n",
    "    summ = 0\n",
    "    for i in docs:\n",
    "        summ += len(i.split())\n",
    "    print(summ/len(docs))\n",
    "    # max_length = len(max(docs, key=len).split())\n",
    "    max_length = 50\n",
    "    ##\n",
    "    padded_docs = pad_sequences(encoded_docs, maxlen=max_length, padding='post')\n",
    "    return padded_docs, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define documents\n",
    "## read test Data\n",
    "def read_test_Data():\n",
    "    SPL = \"<SPL>\"\n",
    "    tes_path = \"csv/trimedValues3.csv\"\n",
    "    test_file = tes_path\n",
    "    dataframe = pd.read_csv(test_file, na_filter = False)\n",
    "    test_docs = []\n",
    "    test_labels = []\n",
    "    for i,data in dataframe.iterrows():\n",
    "        d = data[\"comment\"]\n",
    "        c = data[\"code\"]\n",
    "        s = []\n",
    "        s =  d+\" \"+SPL+\" \"+c # comment and code together\n",
    "        test_docs.append(s)\n",
    "    # integer encode the documents\n",
    "    vocab_size = 2000\n",
    "    test_encoded_docs = [one_hot(d, vocab_size) for d in test_docs]\n",
    "    # pad documents to a max length of 41 words\n",
    "    # The langest sentence contains 200 words but the Average is 21. If  padding all sentence to length of langest one, most of sentence\n",
    "    # will be 0. That is not good for RNN, so we set padding length to 50\n",
    "    summ = 0\n",
    "    for i in test_docs:\n",
    "        summ += len(i.split())\n",
    "#     print(summ/len(test_docs))\n",
    "    # max_length = len(max(docs, key=len).split())\n",
    "    max_length = 50\n",
    "    ##\n",
    "    test_padded_docs = pad_sequences(test_encoded_docs, maxlen=max_length, padding='post')\n",
    "    return test_padded_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21.7673531655225\n",
      "Model: \"model_8\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "x_seq (InputLayer)              (None, 50)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_10 (Embedding)        (None, 50, 32)       64000       x_seq[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_15 (Conv1D)              (None, 49, 10)       650         embedding_10[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_16 (Conv1D)              (None, 48, 10)       970         embedding_10[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_15 (MaxPooling1D) (None, 1, 10)        0           conv1d_15[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_16 (MaxPooling1D) (None, 1, 10)        0           conv1d_16[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "flatten_16 (Flatten)            (None, 10)           0           max_pooling1d_15[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "flatten_17 (Flatten)            (None, 10)           0           max_pooling1d_16[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_8 (Concatenate)     (None, 20)           0           flatten_16[0][0]                 \n",
      "                                                                 flatten_17[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dropout_8 (Dropout)             (None, 20)           0           concatenate_8[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_17 (Dense)                (None, 16)           336         dropout_8[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_18 (Dense)                (None, 1)            17          dense_17[0][0]                   \n",
      "==================================================================================================\n",
      "Total params: 65,973\n",
      "Trainable params: 65,973\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "None\n",
      "Epoch 1/5\n",
      "1311/1311 [==============================] - 1s 912us/step - loss: 0.6388 - accuracy: 0.6812 - precision_10: 0.2405 - recall_10: 0.0504\n",
      "Epoch 2/5\n",
      "1311/1311 [==============================] - 0s 315us/step - loss: 0.5486 - accuracy: 0.7216 - precision_10: 1.0000 - recall_10: 0.0318\n",
      "Epoch 3/5\n",
      "1311/1311 [==============================] - 0s 308us/step - loss: 0.4833 - accuracy: 0.7574 - precision_10: 0.9683 - recall_10: 0.1618\n",
      "Epoch 4/5\n",
      "1311/1311 [==============================] - 0s 314us/step - loss: 0.4144 - accuracy: 0.7994 - precision_10: 0.8750 - recall_10: 0.3528\n",
      "Epoch 5/5\n",
      "1311/1311 [==============================] - 0s 305us/step - loss: 0.3315 - accuracy: 0.8818 - precision_10: 0.8936 - recall_10: 0.6684\n"
     ]
    }
   ],
   "source": [
    "# train CNN with training data\n",
    "\n",
    "padded_docs,labels = read_train_Data()\n",
    "model_CNN = CNN(max_length,vocab_size,32)\n",
    "model_CNN.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy',keras.metrics.Precision(),keras.metrics.Recall()])\n",
    "model_CNN = model_fit_no_test(model_CNN,padded_docs,labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CNN Predicted non-informative  ['no', 'no', 'no', 'no', 'yes', 'no', 'no', 'yes', 'no', 'no', 'no', 'no', 'no', 'no', 'no', 'yes', 'no', 'no', 'no', 'no', 'no', 'no', 'yes', 'no', 'no', 'yes', 'no', 'no', 'no', 'no', 'no', 'yes', 'no', 'no', 'no', 'no', 'no', 'no', 'no', 'no', 'no', 'no', 'no', 'no', 'yes', 'no', 'no', 'no', 'no', 'yes', 'no', 'no', 'no', 'no', 'yes', 'yes', 'yes', 'yes', 'yes', 'no', 'no', 'no', 'no', 'no', 'no', 'no', 'no', 'no', 'no', 'no', 'no', 'no', 'no', 'no', 'no', 'yes', 'no', 'no', 'no', 'no', 'no', 'no', 'no', 'no', 'no', 'no', 'no', 'no', 'no', 'no', 'no', 'no', 'no', 'no', 'yes', 'no', 'yes', 'no', 'no', 'no', 'no', 'no', 'no', 'no', 'no', 'no', 'no', 'no', 'no', 'no', 'yes', 'yes', 'no', 'no', 'no', 'no', 'no', 'no', 'no', 'no', 'no', 'no', 'no', 'no', 'no', 'no', 'no', 'no', 'yes', 'yes', 'yes', 'no', 'no', 'no', 'no', 'yes', 'no', 'yes', 'no', 'no', 'no', 'no', 'no', 'yes', 'no', 'no', 'no', 'no', 'no', 'no', 'no', 'no', 'yes', 'no', 'no', 'no', 'yes', 'no', 'no', 'no', 'no', 'no', 'no', 'no', 'no', 'no', 'no', 'no', 'no', 'no', 'no', 'no', 'no', 'no', 'no', 'no', 'no', 'no', 'no', 'no', 'no', 'no', 'no', 'no', 'no', 'no', 'no', 'yes', 'no', 'no', 'no', 'no', 'no', 'no', 'yes', 'no', 'no', 'no', 'no', 'no', 'no', 'no', 'no', 'no', 'no', 'no', 'no', 'no', 'no', 'no', 'no', 'no', 'no', 'no', 'no', 'no', 'no', 'no', 'no', 'no', 'no', 'no', 'no', 'no', 'no', 'no', 'no', 'yes', 'no', 'no', 'no', 'no', 'no', 'no', 'no', 'no', 'no', 'no', 'no', 'no', 'no', 'yes', 'yes', 'no', 'no', 'no', 'no', 'no', 'no', 'no', 'no', 'no', 'no', 'no', 'no', 'no', 'no', 'no', 'no', 'no', 'no']\n"
     ]
    }
   ],
   "source": [
    "## CNN test\n",
    "test_padded_docs = read_test_Data()\n",
    "outPath = \"result/CNN-Result.txt\"\n",
    "i = []\n",
    "for tr in (test_padded_docs):\n",
    "    i.append(predict_test(model_CNN,tr))\n",
    "print(\"CNN Predicted non-informative \",i)\n",
    "write_result(i, outPath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21.7673531655225\n",
      "Model: \"sequential_5\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_13 (Embedding)     (None, 50, 16)            32000     \n",
      "_________________________________________________________________\n",
      "gru_4 (GRU)                  (None, 16)                1584      \n",
      "_________________________________________________________________\n",
      "dense_21 (Dense)             (None, 1)                 17        \n",
      "=================================================================\n",
      "Total params: 33,601\n",
      "Trainable params: 33,601\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/20\n",
      "1311/1311 [==============================] - 6s 5ms/step - loss: 0.6551 - accuracy: 0.6880 - precision_13: 0.2714 - recall_13: 0.0504\n",
      "Epoch 2/20\n",
      "1311/1311 [==============================] - 5s 4ms/step - loss: 0.6058 - accuracy: 0.7124 - precision_13: 0.0000e+00 - recall_13: 0.0000e+00\n",
      "Epoch 3/20\n",
      "1311/1311 [==============================] - 5s 4ms/step - loss: 0.6031 - accuracy: 0.7124 - precision_13: 0.0000e+00 - recall_13: 0.0000e+00\n",
      "Epoch 4/20\n",
      "1311/1311 [==============================] - 5s 4ms/step - loss: 0.6004 - accuracy: 0.7124 - precision_13: 0.0000e+00 - recall_13: 0.0000e+00\n",
      "Epoch 5/20\n",
      "1311/1311 [==============================] - 5s 4ms/step - loss: 0.5984 - accuracy: 0.7124 - precision_13: 0.0000e+00 - recall_13: 0.0000e+00\n",
      "Epoch 6/20\n",
      "1311/1311 [==============================] - 5s 4ms/step - loss: 0.5944 - accuracy: 0.7124 - precision_13: 0.0000e+00 - recall_13: 0.0000e+00\n",
      "Epoch 7/20\n",
      "1311/1311 [==============================] - 5s 4ms/step - loss: 0.5853 - accuracy: 0.7124 - precision_13: 0.0000e+00 - recall_13: 0.0000e+00\n",
      "Epoch 8/20\n",
      "1311/1311 [==============================] - 5s 4ms/step - loss: 0.5347 - accuracy: 0.7147 - precision_13: 0.5366 - recall_13: 0.0584\n",
      "Epoch 9/20\n",
      "1311/1311 [==============================] - 5s 4ms/step - loss: 0.4711 - accuracy: 0.7513 - precision_13: 0.5859 - recall_13: 0.4615\n",
      "Epoch 10/20\n",
      "1311/1311 [==============================] - 5s 4ms/step - loss: 0.4311 - accuracy: 0.7941 - precision_13: 0.6427 - recall_13: 0.6393\n",
      "Epoch 11/20\n",
      "1311/1311 [==============================] - 5s 4ms/step - loss: 0.3786 - accuracy: 0.8291 - precision_13: 0.6947 - recall_13: 0.7241\n",
      "Epoch 12/20\n",
      "1311/1311 [==============================] - 5s 4ms/step - loss: 0.3297 - accuracy: 0.8642 - precision_13: 0.7375 - recall_13: 0.8196\n",
      "Epoch 13/20\n",
      "1311/1311 [==============================] - 5s 4ms/step - loss: 0.2880 - accuracy: 0.8894 - precision_13: 0.7900 - recall_13: 0.8382\n",
      "Epoch 14/20\n",
      "1311/1311 [==============================] - 5s 4ms/step - loss: 0.2598 - accuracy: 0.9047 - precision_13: 0.7986 - recall_13: 0.8939\n",
      "Epoch 15/20\n",
      "1311/1311 [==============================] - 5s 4ms/step - loss: 0.2366 - accuracy: 0.9176 - precision_13: 0.8241 - recall_13: 0.9072\n",
      "Epoch 16/20\n",
      "1311/1311 [==============================] - 5s 4ms/step - loss: 0.2199 - accuracy: 0.9245 - precision_13: 0.8248 - recall_13: 0.9363\n",
      "Epoch 17/20\n",
      "1311/1311 [==============================] - 5s 4ms/step - loss: 0.2160 - accuracy: 0.9245 - precision_13: 0.8263 - recall_13: 0.9337\n",
      "Epoch 18/20\n",
      "1311/1311 [==============================] - 5s 4ms/step - loss: 0.2013 - accuracy: 0.9329 - precision_13: 0.8416 - recall_13: 0.9443\n",
      "Epoch 19/20\n",
      "1311/1311 [==============================] - 5s 4ms/step - loss: 0.1967 - accuracy: 0.9329 - precision_13: 0.8400 - recall_13: 0.9469\n",
      "Epoch 20/20\n",
      "1311/1311 [==============================] - 5s 4ms/step - loss: 0.1920 - accuracy: 0.9367 - precision_13: 0.8419 - recall_13: 0.9602\n"
     ]
    }
   ],
   "source": [
    "# train RNN with training data\n",
    "padded_docs,labels = read_train_Data()\n",
    "model = RNN(max_length,vocab_size,16)\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy',keras.metrics.Precision(),keras.metrics.Recall()])\n",
    "model = model_fit_no_test(model,padded_docs,labels, 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RNN Predicted non-informative  ['no', 'no', 'no', 'no', 'yes', 'no', 'no', 'yes', 'no', 'no', 'yes', 'yes', 'no', 'no', 'no', 'yes', 'yes', 'no', 'no', 'no', 'no', 'no', 'yes', 'no', 'no', 'yes', 'no', 'no', 'no', 'no', 'no', 'yes', 'no', 'no', 'no', 'no', 'no', 'no', 'no', 'yes', 'no', 'no', 'yes', 'no', 'no', 'no', 'no', 'no', 'no', 'yes', 'no', 'no', 'no', 'no', 'yes', 'yes', 'yes', 'yes', 'yes', 'no', 'no', 'yes', 'no', 'no', 'no', 'no', 'no', 'no', 'no', 'no', 'no', 'no', 'yes', 'no', 'no', 'yes', 'no', 'no', 'yes', 'yes', 'yes', 'no', 'no', 'no', 'yes', 'no', 'no', 'no', 'no', 'no', 'no', 'no', 'no', 'yes', 'yes', 'yes', 'yes', 'yes', 'no', 'no', 'no', 'no', 'no', 'no', 'no', 'no', 'no', 'yes', 'no', 'yes', 'yes', 'no', 'no', 'no', 'no', 'no', 'no', 'no', 'no', 'no', 'no', 'no', 'no', 'yes', 'no', 'no', 'no', 'no', 'yes', 'yes', 'yes', 'no', 'no', 'no', 'no', 'yes', 'no', 'yes', 'no', 'no', 'no', 'no', 'no', 'yes', 'no', 'no', 'no', 'no', 'no', 'no', 'no', 'yes', 'no', 'no', 'no', 'no', 'yes', 'no', 'no', 'no', 'no', 'no', 'no', 'no', 'yes', 'yes', 'no', 'no', 'no', 'no', 'no', 'no', 'yes', 'no', 'yes', 'yes', 'yes', 'no', 'no', 'no', 'yes', 'no', 'no', 'no', 'no', 'no', 'no', 'yes', 'no', 'no', 'no', 'no', 'no', 'no', 'yes', 'no', 'no', 'no', 'no', 'no', 'no', 'no', 'no', 'no', 'no', 'no', 'no', 'no', 'no', 'no', 'no', 'no', 'no', 'no', 'no', 'no', 'no', 'no', 'yes', 'no', 'no', 'no', 'no', 'no', 'no', 'no', 'no', 'yes', 'no', 'no', 'no', 'no', 'no', 'no', 'no', 'no', 'no', 'no', 'no', 'no', 'no', 'yes', 'yes', 'no', 'no', 'no', 'no', 'no', 'no', 'no', 'no', 'yes', 'no', 'no', 'no', 'no', 'no', 'no', 'no', 'no', 'yes']\n"
     ]
    }
   ],
   "source": [
    "## RNN test\n",
    "test_padded_docs = read_test_Data()\n",
    "\n",
    "i = []\n",
    "for tr in (test_padded_docs):\n",
    "    i.append(predict_test(model,tr))\n",
    "print(\"RNN Predicted non-informative \",i)\n",
    "outPath = \"result/RNN-Result.txt\"\n",
    "write_result(i, outPath,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21.7673531655225\n",
      "Model: \"sequential_6\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_14 (Embedding)     (None, 50, 32)            64000     \n",
      "_________________________________________________________________\n",
      "flatten_18 (Flatten)         (None, 1600)              0         \n",
      "_________________________________________________________________\n",
      "dense_22 (Dense)             (None, 1)                 1601      \n",
      "=================================================================\n",
      "Total params: 65,601\n",
      "Trainable params: 65,601\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/5\n",
      "1311/1311 [==============================] - 1s 640us/step - loss: 0.6330 - accuracy: 0.6972 - precision_14: 0.0000e+00 - recall_14: 0.0000e+00\n",
      "Epoch 2/5\n",
      "1311/1311 [==============================] - 0s 194us/step - loss: 0.5432 - accuracy: 0.7429 - precision_14: 0.7273 - recall_14: 0.1698\n",
      "Epoch 3/5\n",
      "1311/1311 [==============================] - 0s 195us/step - loss: 0.4707 - accuracy: 0.7727 - precision_14: 0.6695 - recall_14: 0.4138\n",
      "Epoch 4/5\n",
      "1311/1311 [==============================] - 0s 203us/step - loss: 0.4165 - accuracy: 0.8024 - precision_14: 0.7360 - recall_14: 0.4881\n",
      "Epoch 5/5\n",
      "1311/1311 [==============================] - 0s 199us/step - loss: 0.3666 - accuracy: 0.8314 - precision_14: 0.7847 - recall_14: 0.5703\n"
     ]
    }
   ],
   "source": [
    "# train embedding with training data\n",
    "padded_docs,labels = read_train_Data()\n",
    "model_em = embedding(max_length,vocab_size,32)\n",
    "model_em.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy',keras.metrics.Precision(),keras.metrics.Recall()])\n",
    "model_em = model_fit_no_test(model_em,padded_docs,labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding Predicted non-informative  ['no', 'no', 'no', 'no', 'yes', 'no', 'no', 'yes', 'no', 'no', 'yes', 'yes', 'no', 'no', 'no', 'yes', 'no', 'no', 'no', 'no', 'no', 'no', 'no', 'no', 'no', 'no', 'no', 'no', 'no', 'yes', 'no', 'no', 'no', 'no', 'yes', 'no', 'no', 'no', 'no', 'yes', 'no', 'no', 'yes', 'no', 'no', 'no', 'no', 'no', 'no', 'no', 'no', 'no', 'no', 'no', 'yes', 'yes', 'yes', 'yes', 'yes', 'no', 'no', 'no', 'no', 'no', 'no', 'yes', 'no', 'no', 'no', 'no', 'no', 'no', 'yes', 'no', 'no', 'yes', 'no', 'no', 'no', 'no', 'yes', 'no', 'no', 'no', 'no', 'no', 'no', 'no', 'no', 'no', 'no', 'no', 'no', 'no', 'yes', 'no', 'yes', 'yes', 'no', 'no', 'no', 'no', 'no', 'no', 'no', 'no', 'no', 'no', 'yes', 'yes', 'yes', 'no', 'no', 'no', 'no', 'no', 'no', 'yes', 'yes', 'no', 'no', 'no', 'no', 'yes', 'no', 'no', 'no', 'no', 'yes', 'yes', 'yes', 'no', 'no', 'no', 'no', 'yes', 'no', 'yes', 'no', 'no', 'no', 'no', 'yes', 'yes', 'no', 'no', 'no', 'no', 'no', 'no', 'no', 'yes', 'yes', 'no', 'no', 'no', 'yes', 'no', 'no', 'no', 'no', 'no', 'no', 'no', 'no', 'no', 'no', 'no', 'no', 'no', 'no', 'no', 'no', 'no', 'yes', 'no', 'yes', 'no', 'no', 'no', 'yes', 'no', 'yes', 'yes', 'no', 'no', 'no', 'no', 'no', 'no', 'no', 'no', 'no', 'no', 'yes', 'no', 'no', 'no', 'no', 'no', 'no', 'no', 'no', 'no', 'no', 'no', 'no', 'no', 'no', 'no', 'no', 'no', 'no', 'no', 'no', 'no', 'no', 'no', 'no', 'no', 'no', 'no', 'no', 'no', 'no', 'no', 'no', 'yes', 'no', 'no', 'no', 'no', 'no', 'no', 'no', 'no', 'no', 'no', 'no', 'no', 'no', 'yes', 'yes', 'no', 'no', 'no', 'no', 'no', 'no', 'no', 'no', 'no', 'no', 'no', 'no', 'no', 'no', 'no', 'no', 'no', 'no']\n"
     ]
    }
   ],
   "source": [
    "## embedding test\n",
    "test_padded_docs = read_test_Data()\n",
    "\n",
    "i = []\n",
    "for tr in (test_padded_docs):\n",
    "    i.append(predict_test(model_em,tr))\n",
    "print(\"Embedding Predicted non-informative \",i)\n",
    "outPath = \"result/Embedding-Result.txt\"\n",
    "write_result(i, outPath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "padded_docs, labels = read_train_Data()\n",
    "# CNN train with split \n",
    "train_index = range(1050)\n",
    "test_index = range(1050,len(padded_docs))\n",
    "model_CNN = CNN(max_length,vocab_size,32)\n",
    "model_CNN.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy',keras.metrics.Precision(),keras.metrics.Recall()])\n",
    "model_CNN = model_fit(model_CNN,padded_docs,labels,train_index,test_index)\n",
    "# n_split=5\n",
    "# for train_index,test_index in KFold(n_split).split(padded_docs):\n",
    "#     # define the model\n",
    "#     model = CNN(max_length,vocab_size,32)\n",
    "    \n",
    "#     model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy',keras.metrics.Precision(),keras.metrics.Recall()])\n",
    "\n",
    "#     model = model_fit(model,padded_docs,labels,train_index,test_index)\n",
    "## CNN test-train-result\n",
    "i = 0\n",
    "for f, b in zip(padded_docs[1050:], labels[1050:]):\n",
    "    i += predict_train(model_CNN,f,b)\n",
    "print(\"total num:\",len(padded_docs[1050:]))\n",
    "print(i)\n",
    "\n",
    "\n",
    "# embedding train with split \n",
    "train_index = range(1050)\n",
    "test_index = range(1050,len(padded_docs))\n",
    "model_em = embedding(max_length,vocab_size,32)\n",
    "model_em.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy',keras.metrics.Precision(),keras.metrics.Recall()])\n",
    "model_em = model_fit(model_em,padded_docs,labels,train_index,test_index)\n",
    "i = 0\n",
    "for f, b in zip(padded_docs[1050:], labels[1050:]):\n",
    "    i += predict_train(model_em,f,b)\n",
    "print(\"total num:\",len(padded_docs[1050:]))\n",
    "print(i)\n",
    "\n",
    "\n",
    "# RNN train with split \n",
    "train_index = range(1050)\n",
    "test_index = range(1050,len(padded_docs))\n",
    "model = RNN(max_length,vocab_size,16)\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy',keras.metrics.Precision(),keras.metrics.Recall()])\n",
    "model = model_fit(model,padded_docs,labels,train_index,test_index, 20)\n",
    "## RNN train result\n",
    "i = 0\n",
    "for f, b in zip(padded_docs[1050:], labels[1050:]):\n",
    "    i += predict_train(model,f,b)\n",
    "print(\"total num:\",len(padded_docs[1050:]))\n",
    "print(i)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
