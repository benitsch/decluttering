{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Flatten, Dropout, Bidirectional, TimeDistributed,Conv1D, MaxPooling1D, Input, concatenate\n",
    "from keras.layers.recurrent import SimpleRNN\n",
    "from keras.layers import GRU, LSTM\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.preprocessing import sequence\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.models import Model\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.optimizers import Adam\n",
    "import keras\n",
    "import os\n",
    "import tarfile\n",
    "import re\n",
    "import pandas as pd\n",
    "from numpy import array\n",
    "from keras.preprocessing.text import one_hot\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "import numpy as np\n",
    "from sklearn.model_selection import KFold\n",
    "from keras_self_attention import SeqSelfAttention\n",
    "import statistics\n",
    "\n",
    "max_length = 50\n",
    "vocab_size = 2000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def RNN(maxlen = 50, max_features = 4590, embed_size =32):\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(max_features, embed_size, input_length=maxlen))\n",
    "#     model.add(Dropout(0.5))\n",
    "    model.add(GRU(16,dropout=0.2,return_sequences=True))\n",
    "    model.add(SeqSelfAttention(attention_activation='sigmoid'))\n",
    "    model.add(Flatten())\n",
    "#     model.add(Dense(8, activation='relu'))\n",
    "#     model.add(Dropout(0.5))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    print(model.summary())\n",
    "\n",
    "    return model\n",
    "\n",
    "def CNN(maxlen=50, max_features=4590, embed_size=32):\n",
    "    # Inputs\n",
    "    comment_seq = Input(shape=[maxlen], name='x_seq')\n",
    "\n",
    "    # Embeddings layers\n",
    "    emb_comment = Embedding(max_features, embed_size)(comment_seq)\n",
    "\n",
    "    # conv layers\n",
    "    convs = []\n",
    "#     filter_sizes = [2, 3, 4, 5]\n",
    "    filter_sizes = [2, 3]\n",
    "\n",
    "    for fsz in filter_sizes:\n",
    "        l_conv = Conv1D(filters=10, kernel_size=fsz, activation='relu')(emb_comment)\n",
    "        l_pool = MaxPooling1D(maxlen - fsz + 1)(l_conv)\n",
    "        l_pool = Flatten()(l_pool)\n",
    "        convs.append(l_pool)\n",
    "    merge = concatenate(convs, axis=1)\n",
    "\n",
    "    out = Dropout(0.2)(merge)\n",
    "    output = Dense(16, activation='relu')(out)\n",
    "\n",
    "    output = Dense(units=1, activation='sigmoid')(output)\n",
    "\n",
    "    model = Model([comment_seq], output)\n",
    "    print(model.summary())\n",
    "    return model\n",
    "\n",
    "\n",
    "def embedding(maxlen=50, max_features=4590, embed_size=16):\n",
    "    # define the model\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(max_features, embed_size, input_length=maxlen))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    # summarize the model\n",
    "    print(model.summary())\n",
    "    return model\n",
    "\n",
    "    \n",
    "def model_fit(model,padded_docs,labels,train_index,test_index, epochs = 5):\n",
    "    len_train = len(train_index)\n",
    "    len_test = len(test_index)\n",
    "    x_test =  np.zeros(shape = (len_test,len(padded_docs[0])))\n",
    "    y_test = np.zeros(shape = (len_test))\n",
    "    x_train = np.zeros(shape = (len_train,len(padded_docs[0])))\n",
    "    y_train = np.zeros(shape = (len_train))\n",
    "    i = 0\n",
    "    for tri in train_index:\n",
    "        x_train[i] = padded_docs[tri]\n",
    "        y_train[i] = labels[tri]\n",
    "        i+=1\n",
    "    j = 0\n",
    "    for ti in test_index:\n",
    "        x_test[j] = padded_docs[ti]\n",
    "        y_test[j] =  labels[ti]\n",
    "        j +=1\n",
    "        \n",
    "    print(\"Test set from \",test_index[0], \" to \",test_index[-1])\n",
    "\n",
    "    model.fit(x_train, y_train,epochs=epochs)\n",
    "    eva = model.evaluate(x_test,y_test)\n",
    "    print('loss: ',eva[0])\n",
    "    print('accuracy: ',eva[1])\n",
    "    print('precision: ',eva[2])\n",
    "    print('recall: ',eva[3])\n",
    "    print('f1-score: ',(2*eva[3]*eva[2])/(eva[3]+eva[2]))\n",
    "    return model ,eva\n",
    "\n",
    "def model_fit_no_test(model,x_train,y_train, epochs = 5):\n",
    "    \n",
    "    model.fit(x_train, y_train,epochs=epochs)\n",
    "    return model \n",
    "\n",
    "import numpy\n",
    "def predict_test(model, testData):\n",
    "    test = numpy.reshape(testData,(1,testData.shape[0]))\n",
    "    pr = model.predict(test)\n",
    "    re = []\n",
    "    if (pr >= 0.5):\n",
    "        return (\"yes\")\n",
    "    else:\n",
    "        return (\"no\")\n",
    "    \n",
    "def predict_train(model, testData, truth):\n",
    "    test = numpy.reshape(testData,(1,testData.shape[0]))\n",
    "    pr = model.predict(test)\n",
    "    print(\"==================\")\n",
    "    print(\"truth = \",truth )\n",
    "    print(\"predict = \",pr )\n",
    "    if (pr >= 0.5 and truth == 1) or (pr< 0.5 and truth ==0):\n",
    "        print(\"true\")\n",
    "        return 1\n",
    "    else:\n",
    "        print(\"false\")\n",
    "        return 0\n",
    "    \n",
    "def write_result(predict, outPath,inpath =\"assets/test_set_0520.csv\"):\n",
    "    dataframe_in = pd.read_csv(inpath, na_filter = False)\n",
    "    dataframe_in[\"Expected\"] = predict\n",
    "    dataframe_in.to_csv(outPath)\n",
    "\n",
    "def getMedian(textList):\n",
    "     return statistics.median(textList)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def demostrateData():\n",
    "    SPL = \"<SPL>\"\n",
    "    path = \"csv/trimedValues2.csv\"\n",
    "    train_file = path\n",
    "    dataframe = pd.read_csv(path, na_filter = False)\n",
    "    docs = []\n",
    "    labels = []\n",
    "    for i,data in dataframe.iterrows():\n",
    "        d = data[\"comment\"]\n",
    "        c = data[\"code\"]\n",
    "        s = []\n",
    "        s =  d+\" \"+SPL+\" \"+c # comment and code together\n",
    "        docs.append(s)\n",
    "        l = 1 if data[\"non-information\"] == \"yes\" else 0\n",
    "        labels.append(l)\n",
    "    # integer encode the documents\n",
    "    vocab_size = 2000\n",
    "    encoded_docs = [one_hot(d, vocab_size) for d in docs]\n",
    "    # pad documents to a max length of 41 words\n",
    "    # The langest sentence contains 200 words but the Average is 21. If  padding all sentence to length of langest one, most of sentence\n",
    "    # will be 0. That is not good for RNN, so we set padding length to 50\n",
    "    summ = 0 \n",
    "    li = []\n",
    "    for t in encoded_docs:\n",
    "        li.append(len(t))\n",
    "        summ += len(t)\n",
    "    avg = summ/len(encoded_docs)\n",
    "    print(\"avg =\", avg )\n",
    "    m = getMedian(li)\n",
    "    print(\"median = \",m)\n",
    "    maxLen = len(max(docs, key=len).split())\n",
    "    print(\"max = \",maxLen)\n",
    "    li.sort()\n",
    "    ## plot graph\n",
    "    import matplotlib.pyplot as plt\n",
    "    %matplotlib inline\n",
    "    plt.style.use('ggplot')\n",
    "\n",
    "    x = range(len(encoded_docs))\n",
    "\n",
    "    plt.bar(x, li, color='green')\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "demostrateData()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The most length of given training sentence is less than 50. So we set 50 as padding length. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define documents\n",
    "## read train Data\n",
    "def read_train_Data():\n",
    "    SPL = \"<SPL>\"\n",
    "    path = \"csv/trimedValues2.csv\"\n",
    "    train_file = path\n",
    "    dataframe = pd.read_csv(path, na_filter = False)\n",
    "    docs = []\n",
    "    labels = []\n",
    "    for i,data in dataframe.iterrows():\n",
    "        d = data[\"comment\"]\n",
    "        c = data[\"code\"]\n",
    "        s = []\n",
    "        s =  d+\" \"+SPL+\" \"+c # comment and code together\n",
    "        docs.append(s)\n",
    "        l = 1 if data[\"non-information\"] == \"yes\" else 0\n",
    "        labels.append(l)\n",
    "    # integer encode the documents\n",
    "    vocab_size = 2000\n",
    "    encoded_docs = [one_hot(d, vocab_size) for d in docs]\n",
    "    # pad documents to a max length of 41 words\n",
    "    # The langest sentence contains 200 words but the Average is 21. If  padding all sentence to length of langest one, most of sentence\n",
    "    # will be 0. That is not good for RNN, so we set padding length to 50\n",
    "    li = []\n",
    "    for t in encoded_docs:\n",
    "        li.append(len(t))\n",
    "    m = getMedian(li)\n",
    "    # max_length = len(max(docs, key=len).split())\n",
    "    max_length = 50\n",
    "    ##\n",
    "    padded_docs = pad_sequences(encoded_docs, maxlen=max_length, padding='post')\n",
    "    return padded_docs, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define documents\n",
    "## read test Data\n",
    "def read_test_Data():\n",
    "    SPL = \"<SPL>\"\n",
    "    tes_path = \"csv/trimedValues3.csv\"\n",
    "    test_file = tes_path\n",
    "    dataframe = pd.read_csv(test_file, na_filter = False)\n",
    "    test_docs = []\n",
    "    test_labels = []\n",
    "    for i,data in dataframe.iterrows():\n",
    "        d = data[\"comment\"]\n",
    "        c = data[\"code\"]\n",
    "        s = []\n",
    "        s =  d+\" \"+SPL+\" \"+c # comment and code together\n",
    "        test_docs.append(s)\n",
    "    # integer encode the documents\n",
    "    vocab_size = 2000\n",
    "    test_encoded_docs = [one_hot(d, vocab_size) for d in test_docs]\n",
    "    # pad documents to a max length of 41 words\n",
    "    # The langest sentence contains 200 words but the Average is 21. If  padding all sentence to length of langest one, most of sentence\n",
    "    # will be 0. That is not good for RNN, so we set padding length to 50\n",
    "    summ = 0\n",
    "    for i in test_docs:\n",
    "        summ += len(i.split())\n",
    "#     print(summ/len(test_docs))\n",
    "    # max_length = len(max(docs, key=len).split())\n",
    "    max_length = 50\n",
    "    ##\n",
    "    test_padded_docs = pad_sequences(test_encoded_docs, maxlen=max_length, padding='post')\n",
    "    return test_padded_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train CNN with training data\n",
    "\n",
    "padded_docs,labels = read_train_Data()\n",
    "model_CNN = CNN(max_length,vocab_size,32)\n",
    "model_CNN.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy',keras.metrics.Precision(),keras.metrics.Recall()])\n",
    "model_CNN = model_fit_no_test(model_CNN,padded_docs,labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## CNN test\n",
    "test_padded_docs = read_test_Data()\n",
    "outPath = \"result/CNN-Result.txt\"\n",
    "i = []\n",
    "for tr in (test_padded_docs):\n",
    "    i.append(predict_test(model_CNN,tr))\n",
    "print(\"CNN Predicted non-informative \",i)\n",
    "write_result(i, outPath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train RNN with training data\n",
    "padded_docs,labels = read_train_Data()\n",
    "model = RNN(max_length,vocab_size,32)\n",
    "learning_rate = 0.0002\n",
    "adam = Adam(\n",
    "    learning_rate=learning_rate, beta_1=0.9, beta_2=0.999, epsilon=1e-07, amsgrad=False\n",
    "    )\n",
    "model.compile(optimizer=adam, loss='binary_crossentropy', metrics=['accuracy',keras.metrics.Precision(),keras.metrics.Recall()])\n",
    "model = model_fit_no_test(model,padded_docs,labels, 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## RNN test\n",
    "test_padded_docs = read_test_Data()\n",
    "\n",
    "i = []\n",
    "for tr in (test_padded_docs):\n",
    "    i.append(predict_test(model,tr))\n",
    "print(\"RNN Predicted non-informative \",i)\n",
    "outPath = \"result/RNN-Result.txt\"\n",
    "write_result(i, outPath,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train embedding with training data\n",
    "padded_docs,labels = read_train_Data()\n",
    "model_em = embedding(max_length,vocab_size,32)\n",
    "model_em.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy',keras.metrics.Precision(),keras.metrics.Recall()])\n",
    "model_em = model_fit_no_test(model_em,padded_docs,labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## embedding test\n",
    "test_padded_docs = read_test_Data()\n",
    "\n",
    "i = []\n",
    "for tr in (test_padded_docs):\n",
    "    i.append(predict_test(model_em,tr))\n",
    "print(\"Embedding Predicted non-informative \",i)\n",
    "outPath = \"result/Embedding-Result.txt\"\n",
    "write_result(i, outPath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CNN train with split \n",
    "max_length = 50\n",
    "vocab_size = 2000\n",
    "padded_docs, labels = read_train_Data()\n",
    "i = 0\n",
    "n_split=5\n",
    "loss = 0\n",
    "accuracy = 0\n",
    "precision = 0\n",
    "recall = 0\n",
    "f1_score = 0\n",
    "for train_index,test_index in KFold(n_split).split(padded_docs):\n",
    "    # define the model\n",
    "    model = CNN(max_length,vocab_size,32)\n",
    "    \n",
    "    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy',keras.metrics.Precision(),keras.metrics.Recall()])\n",
    "\n",
    "    model,eva = model_fit(model,padded_docs,labels,train_index,test_index)\n",
    "    loss += eva[0]\n",
    "    accuracy += eva[1]\n",
    "    precision += eva[2]\n",
    "    recall += eva[3]\n",
    "    if precision + recall != 0:\n",
    "        f1_score += (2*eva[3]*eva[2])/(eva[3]+eva[2])\n",
    "\n",
    "print('total loss: ',loss/n_split)\n",
    "print('total accuracy: ',accuracy/n_split)\n",
    "print('total precision: ',precision/n_split)\n",
    "print('total recall: ',recall/n_split)\n",
    "print('total f1-score: ',f1_score/n_split)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# embedding train with split \n",
    "max_length = 50\n",
    "vocab_size = 2000\n",
    "padded_docs, labels = read_train_Data()\n",
    "i = 0\n",
    "n_split=5\n",
    "loss = 0\n",
    "accuracy = 0\n",
    "precision = 0\n",
    "recall = 0\n",
    "f1_score = 0\n",
    "for train_index,test_index in KFold(n_split).split(padded_docs):\n",
    "    # define the model\n",
    "    model = embedding(max_length,vocab_size,32)\n",
    "    \n",
    "    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy',keras.metrics.Precision(),keras.metrics.Recall()])\n",
    "\n",
    "    model,eva = model_fit(model,padded_docs,labels,train_index,test_index)\n",
    "    loss += eva[0]\n",
    "    accuracy += eva[1]\n",
    "    precision += eva[2]\n",
    "    recall += eva[3]\n",
    "    f1_score += (2*eva[3]*eva[2])/(eva[3]+eva[2])\n",
    "\n",
    "print('total loss: ',loss/n_split)\n",
    "print('total accuracy: ',accuracy/n_split)\n",
    "print('total precision: ',precision/n_split)\n",
    "print('total recall: ',recall/n_split)\n",
    "print('total f1-score: ',f1_score/n_split)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RNN train with split \n",
    "max_length = 50\n",
    "vocab_size = 2000\n",
    "\n",
    "padded_docs, labels = read_train_Data()\n",
    "# attention will focus on specific words for example comment: auto generated method stub\n",
    "i = 0\n",
    "n_split=5\n",
    "loss = 0\n",
    "accuracy = 0\n",
    "precision = 0\n",
    "recall = 0\n",
    "f1_score = 0\n",
    "for train_index,test_index in KFold(n_split).split(padded_docs):\n",
    "    # define the model\n",
    "    model = RNN(max_length,vocab_size,32)\n",
    "    learning_rate = 0.0002\n",
    "    adam = Adam(\n",
    "        learning_rate=learning_rate, beta_1=0.9, beta_2=0.999, epsilon=1e-07, amsgrad=False\n",
    "        )\n",
    "    early_stopping_callback = keras.callbacks.EarlyStopping(monitor='val_loss',\n",
    "                                                    min_delta=0,\n",
    "                                                    patience=5,\n",
    "                                                    verbose=0, mode='auto')\n",
    "    model.compile(optimizer=adam, loss='binary_crossentropy', metrics=['accuracy',keras.metrics.Precision(),keras.metrics.Recall()])\n",
    "\n",
    "    model,eva = model_fit(model,padded_docs,labels,train_index,test_index, epochs=20)\n",
    "    loss += eva[0]\n",
    "    accuracy += eva[1]\n",
    "    precision += eva[2]\n",
    "    recall += eva[3]\n",
    "    f1_score += (2*eva[3]*eva[2])/(eva[3]+eva[2])\n",
    "\n",
    "print('total loss: ',loss/n_split)\n",
    "print('total accuracy: ',accuracy/n_split)\n",
    "print('total precision: ',precision/n_split)\n",
    "print('total recall: ',recall/n_split)\n",
    "print('total f1-score: ',f1_score/n_split)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
