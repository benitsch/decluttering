{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import fnmatch\n",
    "import pandas as pd\n",
    "import operator\n",
    "import shutil\n",
    "import re\n",
    "import javalang\n",
    "def create_dev_test_train_split_and_vocabulary(root_path, \n",
    "                                               train_output, \n",
    "                                               vocabFile\n",
    "                                              ):\n",
    "\n",
    "    train_file = ''\n",
    "    dev_file = ''\n",
    "    test_file = ''\n",
    "\n",
    "    word_counts = dict()\n",
    "    \n",
    "    for root, dirnames, filenames in os.walk(root_path):\n",
    "        for filename in fnmatch.filter(filenames, '*.csv'):\n",
    "\n",
    "            path = os.path.join(root, filename)\n",
    "            #print(path)\n",
    "            #print(filename)\n",
    "\n",
    "            if filename.endswith(\"test.csv\"):\n",
    "                test_file = path\n",
    "\n",
    "            elif filename.endswith(\"dev.csv\"):\n",
    "                dev_file = path\n",
    "\n",
    "            else:\n",
    "                path = \"csv/oneLineCode.csv\"\n",
    "                train_file = path\n",
    "                dataframe = pd.read_csv(path, na_filter = False)\n",
    "                for i,data in dataframe.iterrows():\n",
    "                    d = splitComment(data[\"comment\"])\n",
    "                    c = tokenizeJavaCode(data[\"code\"])\n",
    "                    print(i) # row number\n",
    "                    print(\"comment:\")\n",
    "                    print(d) # comment words\n",
    "                    \n",
    "                    print(\"code:\")\n",
    "                    print(c) # code words\n",
    "                    s = []\n",
    "                    s =  d+c # comment and code together\n",
    "                    add_counts(word_counts, s)\n",
    "                with open(path, 'r', encoding='utf-8') as text:\n",
    "                    for line in text:\n",
    "                        add_counts(word_counts, line)\n",
    "\n",
    "    vocabulary = build_vocabulary(word_counts)\n",
    "    print(\"-- VOCABULARY BEGIN --\")\n",
    "    print(vocabulary)\n",
    "    print(\"-- VOCABULARY END --\")\n",
    "    write_vocabulary(vocabulary, vocabFile)\n",
    "\n",
    "    write_processed_dataset(train_file, train_output,vocabFile)\n",
    "#     write_processed_dataset(dev_txt_files, dev_output)\n",
    "#     write_processed_dataset(test_txt_files, test_output)\n",
    "\n",
    "def write_processed_dataset(input_file, output_file, vocabFile):\n",
    "    names = [ 'comment', 'code','non-information']\n",
    "    df = pd.DataFrame()\n",
    "    word_vocabulary = read_vocabulary(vocabFile)\n",
    "    dataframe = pd.read_csv(input_file, na_filter = False)\n",
    "    for i,d in dataframe.iterrows():\n",
    "        comment = []\n",
    "        code = []\n",
    "        label = 1 if d[\"non-information\"] == \"yes\" else 0\n",
    "        for token in splitComment(d[\"comment\"]):\n",
    "            comment.append(word_vocabulary.get(token,0))\n",
    "        for token in tokenizeJavaCode(d[\"code\"]):\n",
    "            code.append(word_vocabulary.get(token,0))\n",
    "        da = [{\n",
    "            \"comment\":comment,\n",
    "            \"code\":code,\n",
    "            \"label\":label,\n",
    "        }]\n",
    "        print(da)\n",
    "        df = df.append(da,ignore_index=True,sort=False)\n",
    "    df.to_csv(output_file, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "delimiters = \"#\", \".\",\",\",\"<b>\",\"</b>\",\"-\",\":\",\"<br>\",\"_\",\"?\",\" \",\";\"\n",
    "def splitComment(string,delimiters = delimiters, maxsplit=0):\n",
    "## replace all web https to https\n",
    "    for l in string.split():\n",
    "        if l.startswith(\"https\"):\n",
    "            string = string.replace(l,\"https\")\n",
    "    ## split string by delimiters\n",
    "    regexPattern = '|'.join(map(re.escape, delimiters))\n",
    "    result =  re.split(regexPattern, string, maxsplit)\n",
    "    ## split string by uppercase\n",
    "    f = []\n",
    "    for r in result:\n",
    "        p =  re.sub(r'((?<=[a-z])[A-Z]|(?<!\\A)[A-Z](?=[a-z]))', r' \\1', r)\n",
    "#         print(p)\n",
    "        for q in p.split():\n",
    "            if q != \"\":\n",
    "                f.append(q)\n",
    "    return f\n",
    "def tokenizeJavaCode(code):\n",
    "    result = []\n",
    "    try:\n",
    "        tokens = list(javalang.tokenizer.tokenize(code))\n",
    "        for token in tokens:\n",
    "            result.append(token.value)\n",
    "        return result\n",
    "    except:\n",
    "        return list(code)\n",
    "#TODO stopwords\n",
    "STOPWORDS_RAW=[\"(?i)or$\", \"(?i)and$\", \"(?i)i$\"]\n",
    "\n",
    "def iterable_to_dict(arr):\n",
    "    print(\"-- DEBUG BEGIN --\")\n",
    "    print(dict((x.strip(), i) for (i, x) in enumerate(arr)))\n",
    "    print(\"-- DEBUG END --\")\n",
    "    return dict((x.strip(), (i+1)) for (i, x) in enumerate(arr))\n",
    "\n",
    "def read_vocabulary(file_name):\n",
    "    with open(file_name, 'r', encoding='utf-8') as f:\n",
    "        return iterable_to_dict(f.readlines())\n",
    "def add_counts(word_counts, line):\n",
    "    stopreg = []\n",
    "    for rawregex in STOPWORDS_RAW:\n",
    "        stopreg.append(re.compile(rawregex))\n",
    "    \n",
    "    print(stopreg)\n",
    "    for w in line:  \n",
    "        if any(regex.match(w) for regex in stopreg):\n",
    "            print(\"Stopword:\",w)\n",
    "            continue\n",
    "\n",
    "        word_counts[w] = word_counts.get(w, 0) + 1\n",
    "END = \"</S>\"\n",
    "UNK = \"<UNK>\"\n",
    "NUM = \"<NUM>\"\n",
    "\n",
    "def dump(d, path):\n",
    "    with open(path, 'w') as f:\n",
    "        for s in d:\n",
    "            f.write(\"%s\\n\" % repr(s))\n",
    "        \n",
    "def write_vocabulary(vocabulary, file_name):\n",
    "    if END not in vocabulary:\n",
    "        vocabulary.append(END)\n",
    "    if UNK not in vocabulary:\n",
    "        vocabulary.append(UNK)\n",
    "\n",
    "    print(\"Vocabulary size (write_vocabulary): %d\" % len(vocabulary))\n",
    "\n",
    "    with open(file_name, 'w', encoding='utf-8') as f:\n",
    "        f.write(\"\\n\".join(vocabulary))\n",
    "MAX_WORD_VOCABULARY_SIZE = 100000\n",
    "MIN_WORD_COUNT_IN_VOCAB = 2\n",
    "MAX_SEQUENCE_LEN = 50\n",
    "def build_vocabulary(word_counts):\n",
    "    return [wc[0] for wc in reversed(sorted(word_counts.items(), key=operator.itemgetter(1))) if wc[1] >= MIN_WORD_COUNT_IN_VOCAB and wc[0] != UNK][:MAX_WORD_VOCABULARY_SIZE] # Unk will be appended to end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "root_path = \"csv\"\n",
    "vocabFile = \"csv/vocab.txt\"\n",
    "create_dev_test_train_split_and_vocabulary(root_path,\"csv/test_out.csv\",vocabFile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframe = pd.read_csv(\"csv/oneLineCode.csv\", na_filter = False)\n",
    "for i,data in dataframe.iterrows():\n",
    "    s =  data[\"comment\"]\n",
    "    c = data[\"code\"]\n",
    "    print(i)\n",
    "    print(\"comment:\")\n",
    "    print(s)\n",
    "    print(\"code:\")\n",
    "    print(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "delimiters = \"#\", \".\",\",\",\"<b>\",\"</b>\",\"-\",\":\",\"<br>\",\"_\",\"?\",\" \",\";\"\n",
    "example = \"stacko#verFlow (c) is a=Wesome... isn't it? D/?DD\"\n",
    "def splitText(delimiters, string, maxsplit=0):\n",
    "#    string.replace()\n",
    "    import re\n",
    "    regexPattern = '|'.join(map(re.escape, delimiters))\n",
    "#     print(regexPattern)\n",
    "    result =  re.split(regexPattern, string, maxsplit)\n",
    "    f = []\n",
    "    for r in result:\n",
    "        p =  re.sub(r'((?<=[a-z])[A-Z]|(?<!\\A)[A-Z](?=[a-z]))', r' \\1', r)\n",
    "#         print(p)\n",
    "        for q in p.split():\n",
    "            if q != \"\":\n",
    "                f.append(q)\n",
    "    return f\n",
    "splitText(delimiters,example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = javalang.tokenizer.tokenize('System.out.println(\"Hello \" + \"world\");')\n",
    "parser = javalang.parser.Parser(tokens)\n",
    "parser.parse_expression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = \"This class monitors a set of files for changes. Upon detecting a change it notifies the registered {@link FileUpdateListener}s. Implementation based on https://stackoverflow.com/questions/16251273/can-i-watch-for-single-file-change-with-watchservice-not-the-whole-directory\"\n",
    "splitComment(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
