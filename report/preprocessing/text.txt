Used tools:
spaCy (https://spacy.io/):
spaCy is an open-source software libaray for advanced natural language processing (NLP), written in the programming languages Pytthon.
We have choosen spaCy, because it is the best way to prepare text for deep learning (https://spacy.io/usage/facts-figures) and several websites and studies (https://www.aclweb.org/anthology/P15-1038.pdf) have shown that spaCy is faster and has better accuracy when tokenizing, than his competitors.
Since only English is important for the challenge, only one of the 55 available language models of spaCy (en_core_web_sm) is needed to analyze the comments and code.
Once you have downloaded and installed the model, you can load it via spacy.load(). This will return a Language object containing all components and data needed to precess text.
When calling the nlp object on a string of text, it will return a processed Doc.
During processing, spaCy first tokenizes the text, i.e. segments it into word, punctuation and so on. This is done by applying rules specific to each language. Each Doc consists of individual tokens, and we can iterate over them.

extract_comment_and_code.ipynb:
This file is necessary to extract the comment and code from the provided Java files, which are downloaded from the download_code.ipynb file.
We first load the csv file via pandas (https://pandas.pydata.org/), which is a software library written for Python for data manipulation and analysis.
Then we loop over each entry and differ between the various comment types (Javadoc, Line, Block). Depending on the type, we extract the comment and code differently.
We take the first code line for which the comment was written for. Sometimes only comment without code exists, but these are a few exceptions. Furthermore, if only a comment without code is provided it is difficult to say whether a comment is useful or not.

preprocess_data.ipynb:
We first load the csv file via pandas (https://pandas.pydata.org/), which is a software library written for Python for data manipulation and analysis.
Then we extract the comment and code column to loop over each row entry.
We first remove words that are taked with "PUNCT" or "DET", which are for e.g. ";", ".", ",", "be", "is", etc. These words have no meaning for the analysis of the comment or code.
Then we pass the word through a few functions to get a clean sentence at the end. First we replace static words like n't (which was tokenized from spaCy) to 'not', or 'll to 'will', or || to 'or'.
After that we check if the word has been written in Camel case and split the word into several words. Since in a comment there was a url written, which does not provide a meaningful information, we pass the words through a function
which removes via regex a given url. Since not every symbol is tagged correct via spaCy, we pass the word through a function which removes predefined symbols from the word (e.g. "#", '(', ')', '{', '}', '[', ']', '\'', '/', '_', '-', '"', '=').
To finally use the sentence with the filtered words for machine learning, we write all words in lower case.
At the end we save all pre-processed comments and code to a new csv file, which can then be used for Machine learning puropses.
